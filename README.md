## TFModelQuantizer

A package which converts saved models weights to quantized FP32 ,FP16 and INT8 for faster Inference through TF-TensorRT. A demonstration of the package on Resnet-50 is provided in the [Colab Notebook](https://colab.research.google.com/drive/1CDKGzLtt2Zy51TWt4bRQou3i7Mvy_THa?usp=sharing)

A detailed overview of the package and its optimizations have been provided in the [Kaggle Notebook](https://www.kaggle.com/abhilash1910/tfmodelquantizer-quantization-of-tf-models/)

<img src="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/graphics/tf-trt-workflow.png">